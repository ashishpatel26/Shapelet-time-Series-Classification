{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aligning discovered shapelets with timeseries.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNRZohkGTH4553QV200YkkR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashishpatel26/Shapelet-time-Series-Classification/blob/main/Aligning_discovered_shapelets_with_timeseries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NF9OegM3yt72",
        "outputId": "e68072b1-cde1-4fa6-b6f2-50f97d79e0fa"
      },
      "source": [
        "pip install tslearn"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tslearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/98/9a67e2869a8b1416eb6e6fd5e69c56f86869980403b18f30bdb5783ade9d/tslearn-0.5.0.5-cp37-cp37m-manylinux2010_x86_64.whl (790kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from tslearn) (0.29.22)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from tslearn) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tslearn) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from tslearn) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from tslearn) (1.0.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from tslearn) (0.51.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->tslearn) (56.1.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->tslearn) (0.34.0)\n",
            "Installing collected packages: tslearn\n",
            "Successfully installed tslearn-0.5.0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7dENWhPyeDI",
        "outputId": "6df54a11-53ae-4eea-850d-704b686f550a"
      },
      "source": [
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tslearn.datasets import CachedDatasets\n",
        "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
        "from tslearn.shapelets import LearningShapelets, grabocka_params_to_shapelet_size_dict\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tslearn/clustering/kmeans.py:17: UserWarning: Scikit-learn <0.24 will be deprecated in a future release of tslearn\n",
            "  \"Scikit-learn <0.24 will be deprecated in a \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaY2fesryoqk"
      },
      "source": [
        "# Set a seed to ensure determinism\n",
        "numpy.random.seed(42)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA3MFC_Xyst_"
      },
      "source": [
        "# Load the Trace dataset\n",
        "X_train, y_train, _, _ = CachedDatasets().load_dataset(\"Trace\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "RRKAXzimy22e",
        "outputId": "b2160efa-5fd5-4670-f9dd-4722fc622cc6"
      },
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(X_train[:,:,0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>235</th>\n",
              "      <th>236</th>\n",
              "      <th>237</th>\n",
              "      <th>238</th>\n",
              "      <th>239</th>\n",
              "      <th>240</th>\n",
              "      <th>241</th>\n",
              "      <th>242</th>\n",
              "      <th>243</th>\n",
              "      <th>244</th>\n",
              "      <th>245</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "      <th>259</th>\n",
              "      <th>260</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.54407</td>\n",
              "      <td>0.65786</td>\n",
              "      <td>0.61124</td>\n",
              "      <td>0.54541</td>\n",
              "      <td>0.53216</td>\n",
              "      <td>0.59301</td>\n",
              "      <td>0.59528</td>\n",
              "      <td>0.53828</td>\n",
              "      <td>0.56343</td>\n",
              "      <td>0.59596</td>\n",
              "      <td>0.63291</td>\n",
              "      <td>0.58465</td>\n",
              "      <td>0.62319</td>\n",
              "      <td>0.60595</td>\n",
              "      <td>0.59037</td>\n",
              "      <td>0.62702</td>\n",
              "      <td>0.61349</td>\n",
              "      <td>0.62305</td>\n",
              "      <td>0.54602</td>\n",
              "      <td>0.64114</td>\n",
              "      <td>0.54939</td>\n",
              "      <td>0.61070</td>\n",
              "      <td>0.57797</td>\n",
              "      <td>0.63567</td>\n",
              "      <td>0.57422</td>\n",
              "      <td>0.60636</td>\n",
              "      <td>0.63805</td>\n",
              "      <td>0.64060</td>\n",
              "      <td>0.60581</td>\n",
              "      <td>0.65487</td>\n",
              "      <td>0.62934</td>\n",
              "      <td>0.67355</td>\n",
              "      <td>0.64083</td>\n",
              "      <td>0.67107</td>\n",
              "      <td>0.59539</td>\n",
              "      <td>0.58943</td>\n",
              "      <td>0.61587</td>\n",
              "      <td>0.66704</td>\n",
              "      <td>0.60380</td>\n",
              "      <td>0.63386</td>\n",
              "      <td>...</td>\n",
              "      <td>0.53556</td>\n",
              "      <td>0.54942</td>\n",
              "      <td>0.55353</td>\n",
              "      <td>0.59538</td>\n",
              "      <td>0.53456</td>\n",
              "      <td>0.53055</td>\n",
              "      <td>0.61195</td>\n",
              "      <td>0.58605</td>\n",
              "      <td>0.54531</td>\n",
              "      <td>0.55091</td>\n",
              "      <td>0.56394</td>\n",
              "      <td>0.60954</td>\n",
              "      <td>0.53654</td>\n",
              "      <td>0.59055</td>\n",
              "      <td>0.55313</td>\n",
              "      <td>0.58819</td>\n",
              "      <td>0.53334</td>\n",
              "      <td>0.58794</td>\n",
              "      <td>0.58297</td>\n",
              "      <td>0.59815</td>\n",
              "      <td>0.60032</td>\n",
              "      <td>0.56658</td>\n",
              "      <td>0.67194</td>\n",
              "      <td>0.62128</td>\n",
              "      <td>0.55515</td>\n",
              "      <td>0.58124</td>\n",
              "      <td>0.67464</td>\n",
              "      <td>0.57607</td>\n",
              "      <td>0.63721</td>\n",
              "      <td>0.57822</td>\n",
              "      <td>0.58823</td>\n",
              "      <td>0.57789</td>\n",
              "      <td>0.59807</td>\n",
              "      <td>0.58369</td>\n",
              "      <td>0.60288</td>\n",
              "      <td>0.55426</td>\n",
              "      <td>0.51415</td>\n",
              "      <td>0.60377</td>\n",
              "      <td>0.59633</td>\n",
              "      <td>0.58322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.76643</td>\n",
              "      <td>0.62463</td>\n",
              "      <td>0.74225</td>\n",
              "      <td>0.66822</td>\n",
              "      <td>0.76612</td>\n",
              "      <td>0.73068</td>\n",
              "      <td>0.65511</td>\n",
              "      <td>0.72933</td>\n",
              "      <td>0.72365</td>\n",
              "      <td>0.70282</td>\n",
              "      <td>0.75617</td>\n",
              "      <td>0.70042</td>\n",
              "      <td>0.68231</td>\n",
              "      <td>0.70894</td>\n",
              "      <td>0.70662</td>\n",
              "      <td>0.77882</td>\n",
              "      <td>0.74962</td>\n",
              "      <td>0.72677</td>\n",
              "      <td>0.79272</td>\n",
              "      <td>0.72516</td>\n",
              "      <td>0.73404</td>\n",
              "      <td>0.69498</td>\n",
              "      <td>0.72110</td>\n",
              "      <td>0.70128</td>\n",
              "      <td>0.69025</td>\n",
              "      <td>0.78826</td>\n",
              "      <td>0.76824</td>\n",
              "      <td>0.74619</td>\n",
              "      <td>0.68759</td>\n",
              "      <td>0.75876</td>\n",
              "      <td>0.69932</td>\n",
              "      <td>0.75046</td>\n",
              "      <td>0.72633</td>\n",
              "      <td>0.69426</td>\n",
              "      <td>0.67118</td>\n",
              "      <td>0.68693</td>\n",
              "      <td>0.78832</td>\n",
              "      <td>0.71995</td>\n",
              "      <td>0.68073</td>\n",
              "      <td>0.66583</td>\n",
              "      <td>...</td>\n",
              "      <td>0.58637</td>\n",
              "      <td>0.65099</td>\n",
              "      <td>0.54138</td>\n",
              "      <td>0.62098</td>\n",
              "      <td>0.65276</td>\n",
              "      <td>0.62315</td>\n",
              "      <td>0.56573</td>\n",
              "      <td>0.62071</td>\n",
              "      <td>0.61428</td>\n",
              "      <td>0.61249</td>\n",
              "      <td>0.64243</td>\n",
              "      <td>0.64173</td>\n",
              "      <td>0.63856</td>\n",
              "      <td>0.69515</td>\n",
              "      <td>0.70527</td>\n",
              "      <td>0.68730</td>\n",
              "      <td>0.63856</td>\n",
              "      <td>0.63982</td>\n",
              "      <td>0.70826</td>\n",
              "      <td>0.68997</td>\n",
              "      <td>0.67374</td>\n",
              "      <td>0.68627</td>\n",
              "      <td>0.63037</td>\n",
              "      <td>0.73046</td>\n",
              "      <td>0.65300</td>\n",
              "      <td>0.68405</td>\n",
              "      <td>0.63268</td>\n",
              "      <td>0.65581</td>\n",
              "      <td>0.66276</td>\n",
              "      <td>0.66764</td>\n",
              "      <td>0.68619</td>\n",
              "      <td>0.68500</td>\n",
              "      <td>0.65401</td>\n",
              "      <td>0.68998</td>\n",
              "      <td>0.64181</td>\n",
              "      <td>0.67280</td>\n",
              "      <td>0.61673</td>\n",
              "      <td>0.72218</td>\n",
              "      <td>0.66786</td>\n",
              "      <td>0.72445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-2.04500</td>\n",
              "      <td>-2.09170</td>\n",
              "      <td>-2.07760</td>\n",
              "      <td>-2.03350</td>\n",
              "      <td>-1.99020</td>\n",
              "      <td>-2.05770</td>\n",
              "      <td>-2.01280</td>\n",
              "      <td>-2.05100</td>\n",
              "      <td>-2.05530</td>\n",
              "      <td>-2.07600</td>\n",
              "      <td>-2.00000</td>\n",
              "      <td>-2.01310</td>\n",
              "      <td>-2.04940</td>\n",
              "      <td>-2.05680</td>\n",
              "      <td>-2.04550</td>\n",
              "      <td>-2.06040</td>\n",
              "      <td>-2.07970</td>\n",
              "      <td>-2.06230</td>\n",
              "      <td>-2.02760</td>\n",
              "      <td>-2.02470</td>\n",
              "      <td>-2.00890</td>\n",
              "      <td>-2.00010</td>\n",
              "      <td>-2.05730</td>\n",
              "      <td>-2.09040</td>\n",
              "      <td>-2.04790</td>\n",
              "      <td>-1.98730</td>\n",
              "      <td>-2.07370</td>\n",
              "      <td>-2.02930</td>\n",
              "      <td>-2.01430</td>\n",
              "      <td>-2.04460</td>\n",
              "      <td>-2.05130</td>\n",
              "      <td>-2.02020</td>\n",
              "      <td>-2.03140</td>\n",
              "      <td>-1.99820</td>\n",
              "      <td>-2.01030</td>\n",
              "      <td>-2.03270</td>\n",
              "      <td>-2.03200</td>\n",
              "      <td>-2.00620</td>\n",
              "      <td>-2.01700</td>\n",
              "      <td>-2.01250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.49396</td>\n",
              "      <td>0.55607</td>\n",
              "      <td>0.52739</td>\n",
              "      <td>0.52352</td>\n",
              "      <td>0.50650</td>\n",
              "      <td>0.54799</td>\n",
              "      <td>0.57457</td>\n",
              "      <td>0.58874</td>\n",
              "      <td>0.60291</td>\n",
              "      <td>0.54685</td>\n",
              "      <td>0.50110</td>\n",
              "      <td>0.61613</td>\n",
              "      <td>0.57177</td>\n",
              "      <td>0.56606</td>\n",
              "      <td>0.56036</td>\n",
              "      <td>0.54695</td>\n",
              "      <td>0.54060</td>\n",
              "      <td>0.53820</td>\n",
              "      <td>0.58579</td>\n",
              "      <td>0.54910</td>\n",
              "      <td>0.57988</td>\n",
              "      <td>0.57966</td>\n",
              "      <td>0.49634</td>\n",
              "      <td>0.55481</td>\n",
              "      <td>0.52731</td>\n",
              "      <td>0.49981</td>\n",
              "      <td>0.52698</td>\n",
              "      <td>0.53935</td>\n",
              "      <td>0.55171</td>\n",
              "      <td>0.52500</td>\n",
              "      <td>0.55547</td>\n",
              "      <td>0.52443</td>\n",
              "      <td>0.48541</td>\n",
              "      <td>0.55053</td>\n",
              "      <td>0.59755</td>\n",
              "      <td>0.60643</td>\n",
              "      <td>0.52293</td>\n",
              "      <td>0.52429</td>\n",
              "      <td>0.54742</td>\n",
              "      <td>0.57056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.70500</td>\n",
              "      <td>-1.73910</td>\n",
              "      <td>-1.72570</td>\n",
              "      <td>-1.80170</td>\n",
              "      <td>-1.74250</td>\n",
              "      <td>-1.76040</td>\n",
              "      <td>-1.69160</td>\n",
              "      <td>-1.68750</td>\n",
              "      <td>-1.76150</td>\n",
              "      <td>-1.77480</td>\n",
              "      <td>-1.78210</td>\n",
              "      <td>-1.76500</td>\n",
              "      <td>-1.73770</td>\n",
              "      <td>-1.68860</td>\n",
              "      <td>-1.74840</td>\n",
              "      <td>-1.72520</td>\n",
              "      <td>-1.77680</td>\n",
              "      <td>-1.78020</td>\n",
              "      <td>-1.74530</td>\n",
              "      <td>-1.73300</td>\n",
              "      <td>-1.76500</td>\n",
              "      <td>-1.75840</td>\n",
              "      <td>-1.75540</td>\n",
              "      <td>-1.80480</td>\n",
              "      <td>-1.74280</td>\n",
              "      <td>-1.75240</td>\n",
              "      <td>-1.77440</td>\n",
              "      <td>-1.76230</td>\n",
              "      <td>-1.68320</td>\n",
              "      <td>-1.75850</td>\n",
              "      <td>-1.70970</td>\n",
              "      <td>-1.77700</td>\n",
              "      <td>-1.74240</td>\n",
              "      <td>-1.74640</td>\n",
              "      <td>-1.75010</td>\n",
              "      <td>-1.71470</td>\n",
              "      <td>-1.75490</td>\n",
              "      <td>-1.76950</td>\n",
              "      <td>-1.78800</td>\n",
              "      <td>-1.76940</td>\n",
              "      <td>...</td>\n",
              "      <td>0.60747</td>\n",
              "      <td>0.58897</td>\n",
              "      <td>0.57047</td>\n",
              "      <td>0.63402</td>\n",
              "      <td>0.57357</td>\n",
              "      <td>0.63100</td>\n",
              "      <td>0.64216</td>\n",
              "      <td>0.62261</td>\n",
              "      <td>0.66491</td>\n",
              "      <td>0.66699</td>\n",
              "      <td>0.63428</td>\n",
              "      <td>0.63029</td>\n",
              "      <td>0.66920</td>\n",
              "      <td>0.58981</td>\n",
              "      <td>0.66506</td>\n",
              "      <td>0.61816</td>\n",
              "      <td>0.62845</td>\n",
              "      <td>0.58216</td>\n",
              "      <td>0.57262</td>\n",
              "      <td>0.58582</td>\n",
              "      <td>0.62858</td>\n",
              "      <td>0.66143</td>\n",
              "      <td>0.61961</td>\n",
              "      <td>0.66975</td>\n",
              "      <td>0.61826</td>\n",
              "      <td>0.60868</td>\n",
              "      <td>0.64777</td>\n",
              "      <td>0.60938</td>\n",
              "      <td>0.63675</td>\n",
              "      <td>0.61424</td>\n",
              "      <td>0.64928</td>\n",
              "      <td>0.64069</td>\n",
              "      <td>0.62599</td>\n",
              "      <td>0.61621</td>\n",
              "      <td>0.61688</td>\n",
              "      <td>0.65652</td>\n",
              "      <td>0.59599</td>\n",
              "      <td>0.60164</td>\n",
              "      <td>0.65616</td>\n",
              "      <td>0.58226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.54137</td>\n",
              "      <td>0.57191</td>\n",
              "      <td>0.56215</td>\n",
              "      <td>0.52095</td>\n",
              "      <td>0.52749</td>\n",
              "      <td>0.60404</td>\n",
              "      <td>0.54052</td>\n",
              "      <td>0.58038</td>\n",
              "      <td>0.50058</td>\n",
              "      <td>0.59781</td>\n",
              "      <td>0.55532</td>\n",
              "      <td>0.50400</td>\n",
              "      <td>0.48722</td>\n",
              "      <td>0.52919</td>\n",
              "      <td>0.57732</td>\n",
              "      <td>0.58307</td>\n",
              "      <td>0.56387</td>\n",
              "      <td>0.51737</td>\n",
              "      <td>0.55365</td>\n",
              "      <td>0.63541</td>\n",
              "      <td>0.52845</td>\n",
              "      <td>0.56115</td>\n",
              "      <td>0.50573</td>\n",
              "      <td>0.64667</td>\n",
              "      <td>0.56248</td>\n",
              "      <td>0.56587</td>\n",
              "      <td>0.57505</td>\n",
              "      <td>0.61367</td>\n",
              "      <td>0.59095</td>\n",
              "      <td>0.54807</td>\n",
              "      <td>0.53636</td>\n",
              "      <td>0.58289</td>\n",
              "      <td>0.62480</td>\n",
              "      <td>0.54263</td>\n",
              "      <td>0.53727</td>\n",
              "      <td>0.54421</td>\n",
              "      <td>0.46517</td>\n",
              "      <td>0.51661</td>\n",
              "      <td>0.56632</td>\n",
              "      <td>0.58852</td>\n",
              "      <td>...</td>\n",
              "      <td>0.49039</td>\n",
              "      <td>0.55445</td>\n",
              "      <td>0.55328</td>\n",
              "      <td>0.57360</td>\n",
              "      <td>0.53077</td>\n",
              "      <td>0.53897</td>\n",
              "      <td>0.51796</td>\n",
              "      <td>0.52855</td>\n",
              "      <td>0.51671</td>\n",
              "      <td>0.59622</td>\n",
              "      <td>0.61819</td>\n",
              "      <td>0.57297</td>\n",
              "      <td>0.55145</td>\n",
              "      <td>0.50723</td>\n",
              "      <td>0.55504</td>\n",
              "      <td>0.54870</td>\n",
              "      <td>0.51063</td>\n",
              "      <td>0.50387</td>\n",
              "      <td>0.52631</td>\n",
              "      <td>0.56185</td>\n",
              "      <td>0.55370</td>\n",
              "      <td>0.55088</td>\n",
              "      <td>0.50170</td>\n",
              "      <td>0.55613</td>\n",
              "      <td>0.50657</td>\n",
              "      <td>0.52409</td>\n",
              "      <td>0.52285</td>\n",
              "      <td>0.59391</td>\n",
              "      <td>0.56874</td>\n",
              "      <td>0.54726</td>\n",
              "      <td>0.58600</td>\n",
              "      <td>0.62647</td>\n",
              "      <td>0.60962</td>\n",
              "      <td>0.55428</td>\n",
              "      <td>0.53947</td>\n",
              "      <td>0.53352</td>\n",
              "      <td>0.55775</td>\n",
              "      <td>0.50783</td>\n",
              "      <td>0.51377</td>\n",
              "      <td>0.52813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0.60653</td>\n",
              "      <td>0.62571</td>\n",
              "      <td>0.63304</td>\n",
              "      <td>0.62139</td>\n",
              "      <td>0.60973</td>\n",
              "      <td>0.56836</td>\n",
              "      <td>0.61511</td>\n",
              "      <td>0.62784</td>\n",
              "      <td>0.61174</td>\n",
              "      <td>0.56173</td>\n",
              "      <td>0.59762</td>\n",
              "      <td>0.63602</td>\n",
              "      <td>0.65279</td>\n",
              "      <td>0.59590</td>\n",
              "      <td>0.66719</td>\n",
              "      <td>0.62529</td>\n",
              "      <td>0.59222</td>\n",
              "      <td>0.61863</td>\n",
              "      <td>0.63375</td>\n",
              "      <td>0.63352</td>\n",
              "      <td>0.61728</td>\n",
              "      <td>0.60104</td>\n",
              "      <td>0.61073</td>\n",
              "      <td>0.63133</td>\n",
              "      <td>0.65192</td>\n",
              "      <td>0.65527</td>\n",
              "      <td>0.60080</td>\n",
              "      <td>0.60083</td>\n",
              "      <td>0.58305</td>\n",
              "      <td>0.63802</td>\n",
              "      <td>0.64006</td>\n",
              "      <td>0.58849</td>\n",
              "      <td>0.59515</td>\n",
              "      <td>0.65284</td>\n",
              "      <td>0.63752</td>\n",
              "      <td>0.60446</td>\n",
              "      <td>0.60445</td>\n",
              "      <td>0.60518</td>\n",
              "      <td>0.68358</td>\n",
              "      <td>0.67411</td>\n",
              "      <td>...</td>\n",
              "      <td>0.53940</td>\n",
              "      <td>0.47076</td>\n",
              "      <td>0.45985</td>\n",
              "      <td>0.55102</td>\n",
              "      <td>0.53644</td>\n",
              "      <td>0.58661</td>\n",
              "      <td>0.51349</td>\n",
              "      <td>0.51878</td>\n",
              "      <td>0.55152</td>\n",
              "      <td>0.52052</td>\n",
              "      <td>0.50081</td>\n",
              "      <td>0.52896</td>\n",
              "      <td>0.55711</td>\n",
              "      <td>0.50131</td>\n",
              "      <td>0.64851</td>\n",
              "      <td>0.55347</td>\n",
              "      <td>0.60981</td>\n",
              "      <td>0.57945</td>\n",
              "      <td>0.57919</td>\n",
              "      <td>0.54741</td>\n",
              "      <td>0.52965</td>\n",
              "      <td>0.60046</td>\n",
              "      <td>0.61611</td>\n",
              "      <td>0.54581</td>\n",
              "      <td>0.53650</td>\n",
              "      <td>0.60991</td>\n",
              "      <td>0.56637</td>\n",
              "      <td>0.59683</td>\n",
              "      <td>0.55427</td>\n",
              "      <td>0.56243</td>\n",
              "      <td>0.57223</td>\n",
              "      <td>0.65894</td>\n",
              "      <td>0.55759</td>\n",
              "      <td>0.53845</td>\n",
              "      <td>0.54657</td>\n",
              "      <td>0.55469</td>\n",
              "      <td>0.56967</td>\n",
              "      <td>0.58465</td>\n",
              "      <td>0.63383</td>\n",
              "      <td>0.55859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0.74781</td>\n",
              "      <td>0.76297</td>\n",
              "      <td>0.73012</td>\n",
              "      <td>0.68065</td>\n",
              "      <td>0.77179</td>\n",
              "      <td>0.66561</td>\n",
              "      <td>0.73768</td>\n",
              "      <td>0.76279</td>\n",
              "      <td>0.72582</td>\n",
              "      <td>0.73755</td>\n",
              "      <td>0.71881</td>\n",
              "      <td>0.72987</td>\n",
              "      <td>0.71423</td>\n",
              "      <td>0.75668</td>\n",
              "      <td>0.77767</td>\n",
              "      <td>0.70894</td>\n",
              "      <td>0.72459</td>\n",
              "      <td>0.69873</td>\n",
              "      <td>0.73327</td>\n",
              "      <td>0.69836</td>\n",
              "      <td>0.72162</td>\n",
              "      <td>0.70352</td>\n",
              "      <td>0.72963</td>\n",
              "      <td>0.70811</td>\n",
              "      <td>0.68522</td>\n",
              "      <td>0.71747</td>\n",
              "      <td>0.70508</td>\n",
              "      <td>0.68105</td>\n",
              "      <td>0.66956</td>\n",
              "      <td>0.71797</td>\n",
              "      <td>0.77575</td>\n",
              "      <td>0.72803</td>\n",
              "      <td>0.72534</td>\n",
              "      <td>0.71329</td>\n",
              "      <td>0.71249</td>\n",
              "      <td>0.72658</td>\n",
              "      <td>0.75869</td>\n",
              "      <td>0.73054</td>\n",
              "      <td>0.73244</td>\n",
              "      <td>0.70563</td>\n",
              "      <td>...</td>\n",
              "      <td>0.64205</td>\n",
              "      <td>0.65414</td>\n",
              "      <td>0.62738</td>\n",
              "      <td>0.69403</td>\n",
              "      <td>0.65664</td>\n",
              "      <td>0.69621</td>\n",
              "      <td>0.62705</td>\n",
              "      <td>0.66537</td>\n",
              "      <td>0.65690</td>\n",
              "      <td>0.69800</td>\n",
              "      <td>0.61585</td>\n",
              "      <td>0.69905</td>\n",
              "      <td>0.68429</td>\n",
              "      <td>0.64323</td>\n",
              "      <td>0.66963</td>\n",
              "      <td>0.66669</td>\n",
              "      <td>0.66330</td>\n",
              "      <td>0.61409</td>\n",
              "      <td>0.69680</td>\n",
              "      <td>0.71945</td>\n",
              "      <td>0.64361</td>\n",
              "      <td>0.64510</td>\n",
              "      <td>0.67524</td>\n",
              "      <td>0.66730</td>\n",
              "      <td>0.68141</td>\n",
              "      <td>0.64302</td>\n",
              "      <td>0.66357</td>\n",
              "      <td>0.65146</td>\n",
              "      <td>0.67165</td>\n",
              "      <td>0.67967</td>\n",
              "      <td>0.75198</td>\n",
              "      <td>0.64260</td>\n",
              "      <td>0.78138</td>\n",
              "      <td>0.76449</td>\n",
              "      <td>0.74109</td>\n",
              "      <td>0.66191</td>\n",
              "      <td>0.68592</td>\n",
              "      <td>0.71661</td>\n",
              "      <td>0.67864</td>\n",
              "      <td>0.72428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>-1.26570</td>\n",
              "      <td>-1.23810</td>\n",
              "      <td>-1.25690</td>\n",
              "      <td>-1.26400</td>\n",
              "      <td>-1.25400</td>\n",
              "      <td>-1.22490</td>\n",
              "      <td>-1.28120</td>\n",
              "      <td>-1.26190</td>\n",
              "      <td>-1.28450</td>\n",
              "      <td>-1.25290</td>\n",
              "      <td>-1.23180</td>\n",
              "      <td>-1.24970</td>\n",
              "      <td>-1.23740</td>\n",
              "      <td>-1.26920</td>\n",
              "      <td>-1.26730</td>\n",
              "      <td>-1.22170</td>\n",
              "      <td>-1.23520</td>\n",
              "      <td>-1.26450</td>\n",
              "      <td>-1.26390</td>\n",
              "      <td>-1.23340</td>\n",
              "      <td>-1.20300</td>\n",
              "      <td>-1.23670</td>\n",
              "      <td>-1.23060</td>\n",
              "      <td>-1.26480</td>\n",
              "      <td>-1.27790</td>\n",
              "      <td>-1.21320</td>\n",
              "      <td>-1.27130</td>\n",
              "      <td>-1.25910</td>\n",
              "      <td>-1.19990</td>\n",
              "      <td>-1.24280</td>\n",
              "      <td>-1.24490</td>\n",
              "      <td>-1.24690</td>\n",
              "      <td>-1.21380</td>\n",
              "      <td>-1.29450</td>\n",
              "      <td>-1.21420</td>\n",
              "      <td>-1.24200</td>\n",
              "      <td>-1.25370</td>\n",
              "      <td>-1.26690</td>\n",
              "      <td>-1.21430</td>\n",
              "      <td>-1.25710</td>\n",
              "      <td>...</td>\n",
              "      <td>0.87169</td>\n",
              "      <td>0.86300</td>\n",
              "      <td>0.85948</td>\n",
              "      <td>0.82538</td>\n",
              "      <td>0.88802</td>\n",
              "      <td>0.84851</td>\n",
              "      <td>0.84253</td>\n",
              "      <td>0.89855</td>\n",
              "      <td>0.86780</td>\n",
              "      <td>0.88816</td>\n",
              "      <td>0.88643</td>\n",
              "      <td>0.89005</td>\n",
              "      <td>0.86274</td>\n",
              "      <td>0.86091</td>\n",
              "      <td>0.83009</td>\n",
              "      <td>0.89933</td>\n",
              "      <td>0.84157</td>\n",
              "      <td>0.84669</td>\n",
              "      <td>0.90300</td>\n",
              "      <td>0.87239</td>\n",
              "      <td>0.82839</td>\n",
              "      <td>0.87152</td>\n",
              "      <td>0.84847</td>\n",
              "      <td>0.82543</td>\n",
              "      <td>0.87693</td>\n",
              "      <td>0.86935</td>\n",
              "      <td>0.86667</td>\n",
              "      <td>0.86659</td>\n",
              "      <td>0.84325</td>\n",
              "      <td>0.90158</td>\n",
              "      <td>0.92342</td>\n",
              "      <td>0.82926</td>\n",
              "      <td>0.87544</td>\n",
              "      <td>0.87019</td>\n",
              "      <td>0.82596</td>\n",
              "      <td>0.84466</td>\n",
              "      <td>0.84553</td>\n",
              "      <td>0.90231</td>\n",
              "      <td>0.87317</td>\n",
              "      <td>0.84577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.67424</td>\n",
              "      <td>0.69950</td>\n",
              "      <td>0.72477</td>\n",
              "      <td>0.73695</td>\n",
              "      <td>0.67882</td>\n",
              "      <td>0.66796</td>\n",
              "      <td>0.66676</td>\n",
              "      <td>0.62970</td>\n",
              "      <td>0.61887</td>\n",
              "      <td>0.67119</td>\n",
              "      <td>0.63880</td>\n",
              "      <td>0.66199</td>\n",
              "      <td>0.73420</td>\n",
              "      <td>0.70406</td>\n",
              "      <td>0.67647</td>\n",
              "      <td>0.67090</td>\n",
              "      <td>0.65571</td>\n",
              "      <td>0.64722</td>\n",
              "      <td>0.62853</td>\n",
              "      <td>0.67228</td>\n",
              "      <td>0.71821</td>\n",
              "      <td>0.69228</td>\n",
              "      <td>0.66634</td>\n",
              "      <td>0.62492</td>\n",
              "      <td>0.65319</td>\n",
              "      <td>0.69925</td>\n",
              "      <td>0.71718</td>\n",
              "      <td>0.64166</td>\n",
              "      <td>0.67277</td>\n",
              "      <td>0.64540</td>\n",
              "      <td>0.63094</td>\n",
              "      <td>0.66571</td>\n",
              "      <td>0.74639</td>\n",
              "      <td>0.66863</td>\n",
              "      <td>0.67801</td>\n",
              "      <td>0.70017</td>\n",
              "      <td>0.66641</td>\n",
              "      <td>0.68396</td>\n",
              "      <td>0.65205</td>\n",
              "      <td>0.71873</td>\n",
              "      <td>...</td>\n",
              "      <td>0.44291</td>\n",
              "      <td>0.45311</td>\n",
              "      <td>0.46172</td>\n",
              "      <td>0.47032</td>\n",
              "      <td>0.48955</td>\n",
              "      <td>0.51009</td>\n",
              "      <td>0.54170</td>\n",
              "      <td>0.54733</td>\n",
              "      <td>0.51410</td>\n",
              "      <td>0.57991</td>\n",
              "      <td>0.50412</td>\n",
              "      <td>0.53561</td>\n",
              "      <td>0.54610</td>\n",
              "      <td>0.57103</td>\n",
              "      <td>0.56951</td>\n",
              "      <td>0.56799</td>\n",
              "      <td>0.60128</td>\n",
              "      <td>0.54882</td>\n",
              "      <td>0.56180</td>\n",
              "      <td>0.54972</td>\n",
              "      <td>0.55390</td>\n",
              "      <td>0.55931</td>\n",
              "      <td>0.64079</td>\n",
              "      <td>0.65336</td>\n",
              "      <td>0.63377</td>\n",
              "      <td>0.60597</td>\n",
              "      <td>0.60750</td>\n",
              "      <td>0.63348</td>\n",
              "      <td>0.57768</td>\n",
              "      <td>0.63345</td>\n",
              "      <td>0.63183</td>\n",
              "      <td>0.63022</td>\n",
              "      <td>0.64352</td>\n",
              "      <td>0.60495</td>\n",
              "      <td>0.61276</td>\n",
              "      <td>0.60509</td>\n",
              "      <td>0.59466</td>\n",
              "      <td>0.68093</td>\n",
              "      <td>0.61272</td>\n",
              "      <td>0.65020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.76816</td>\n",
              "      <td>0.71678</td>\n",
              "      <td>0.76064</td>\n",
              "      <td>0.77676</td>\n",
              "      <td>0.78370</td>\n",
              "      <td>0.79063</td>\n",
              "      <td>0.82519</td>\n",
              "      <td>0.69679</td>\n",
              "      <td>0.76291</td>\n",
              "      <td>0.73746</td>\n",
              "      <td>0.73392</td>\n",
              "      <td>0.72121</td>\n",
              "      <td>0.75357</td>\n",
              "      <td>0.81007</td>\n",
              "      <td>0.69666</td>\n",
              "      <td>0.79571</td>\n",
              "      <td>0.77501</td>\n",
              "      <td>0.78910</td>\n",
              "      <td>0.71276</td>\n",
              "      <td>0.80569</td>\n",
              "      <td>0.77720</td>\n",
              "      <td>0.75728</td>\n",
              "      <td>0.69852</td>\n",
              "      <td>0.82294</td>\n",
              "      <td>0.76024</td>\n",
              "      <td>0.70059</td>\n",
              "      <td>0.69918</td>\n",
              "      <td>0.70885</td>\n",
              "      <td>0.75027</td>\n",
              "      <td>0.71063</td>\n",
              "      <td>0.77460</td>\n",
              "      <td>0.76362</td>\n",
              "      <td>0.77063</td>\n",
              "      <td>0.82454</td>\n",
              "      <td>0.78232</td>\n",
              "      <td>0.74010</td>\n",
              "      <td>0.71122</td>\n",
              "      <td>0.72470</td>\n",
              "      <td>0.75618</td>\n",
              "      <td>0.70610</td>\n",
              "      <td>...</td>\n",
              "      <td>0.73253</td>\n",
              "      <td>0.78905</td>\n",
              "      <td>0.75193</td>\n",
              "      <td>0.72334</td>\n",
              "      <td>0.69747</td>\n",
              "      <td>0.76103</td>\n",
              "      <td>0.68805</td>\n",
              "      <td>0.70315</td>\n",
              "      <td>0.67936</td>\n",
              "      <td>0.77012</td>\n",
              "      <td>0.72070</td>\n",
              "      <td>0.67586</td>\n",
              "      <td>0.75017</td>\n",
              "      <td>0.70959</td>\n",
              "      <td>0.69121</td>\n",
              "      <td>0.71458</td>\n",
              "      <td>0.72609</td>\n",
              "      <td>0.72862</td>\n",
              "      <td>0.81017</td>\n",
              "      <td>0.82500</td>\n",
              "      <td>0.74869</td>\n",
              "      <td>0.78302</td>\n",
              "      <td>0.81735</td>\n",
              "      <td>0.76423</td>\n",
              "      <td>0.74728</td>\n",
              "      <td>0.77216</td>\n",
              "      <td>0.75851</td>\n",
              "      <td>0.75808</td>\n",
              "      <td>0.68913</td>\n",
              "      <td>0.76626</td>\n",
              "      <td>0.75080</td>\n",
              "      <td>0.75903</td>\n",
              "      <td>0.70383</td>\n",
              "      <td>0.74529</td>\n",
              "      <td>0.76252</td>\n",
              "      <td>0.78063</td>\n",
              "      <td>0.74059</td>\n",
              "      <td>0.76741</td>\n",
              "      <td>0.75562</td>\n",
              "      <td>0.67906</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 275 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0        1        2        3    ...      271      272      273      274\n",
              "0   0.54407  0.65786  0.61124  0.54541  ...  0.51415  0.60377  0.59633  0.58322\n",
              "1   0.76643  0.62463  0.74225  0.66822  ...  0.61673  0.72218  0.66786  0.72445\n",
              "2  -2.04500 -2.09170 -2.07760 -2.03350  ...  0.52293  0.52429  0.54742  0.57056\n",
              "3  -1.70500 -1.73910 -1.72570 -1.80170  ...  0.59599  0.60164  0.65616  0.58226\n",
              "4   0.54137  0.57191  0.56215  0.52095  ...  0.55775  0.50783  0.51377  0.52813\n",
              "..      ...      ...      ...      ...  ...      ...      ...      ...      ...\n",
              "95  0.60653  0.62571  0.63304  0.62139  ...  0.56967  0.58465  0.63383  0.55859\n",
              "96  0.74781  0.76297  0.73012  0.68065  ...  0.68592  0.71661  0.67864  0.72428\n",
              "97 -1.26570 -1.23810 -1.25690 -1.26400  ...  0.84553  0.90231  0.87317  0.84577\n",
              "98  0.67424  0.69950  0.72477  0.73695  ...  0.59466  0.68093  0.61272  0.65020\n",
              "99  0.76816  0.71678  0.76064  0.77676  ...  0.74059  0.76741  0.75562  0.67906\n",
              "\n",
              "[100 rows x 275 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5cpW0przA5o",
        "outputId": "b6854f31-dcb5-46aa-98a9-7d9630dce441"
      },
      "source": [
        "pd.DataFrame(y_train).value_counts()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4    31\n",
              "1    26\n",
              "3    22\n",
              "2    21\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNMMRmilzGCW"
      },
      "source": [
        "# Filter out classes 2 and 4\n",
        "mask = numpy.isin(y_train, [1, 3])\n",
        "X_train = X_train[mask]\n",
        "y_train = y_train[mask]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPsZ41h6zbDP"
      },
      "source": [
        "# Normalize the time series\n",
        "X_train = TimeSeriesScalerMinMax().fit_transform(X_train)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnD9A1gEzeh9"
      },
      "source": [
        "# Get statistics of the dataset\n",
        "n_ts, ts_sz = X_train.shape[:2]\n",
        "n_classes = len(set(y_train))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTtP-Q40zhdb"
      },
      "source": [
        "# We will extract 1 shapelet and align it with a time series\n",
        "shapelet_sizes = {20: 1}"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClTYmy8EzlqM",
        "outputId": "59196b1b-f191-478b-8900-08ab92d775f1"
      },
      "source": [
        "# Define the model and fit it using the training data\n",
        "shp_clf = LearningShapelets(n_shapelets_per_size=shapelet_sizes,\n",
        "                            weight_regularizer=0.001,\n",
        "                            optimizer=Adam(lr=0.01),\n",
        "                            max_iter=250,\n",
        "                            verbose=1,\n",
        "                            scale=False,\n",
        "                            random_state=42)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tslearn/shapelets/shapelets.py:357: FutureWarning: The default value for 'scale' is set to False in version 0.4 to ensure backward compatibility, but is likely to change in a future version.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJX-jzz2zoZC",
        "outputId": "a4b8a593-b678-4fb2-899d-88ed5504e803"
      },
      "source": [
        "shp_clf.fit(X_train, y_train)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6884 - binary_accuracy: 0.4583 - binary_crossentropy: 0.6880\n",
            "Epoch 2/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6874 - binary_accuracy: 1.0000 - binary_crossentropy: 0.6870\n",
            "Epoch 3/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6864 - binary_accuracy: 1.0000 - binary_crossentropy: 0.6860\n",
            "Epoch 4/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6854 - binary_accuracy: 0.8125 - binary_crossentropy: 0.6850\n",
            "Epoch 5/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6844 - binary_accuracy: 0.5625 - binary_crossentropy: 0.6840\n",
            "Epoch 6/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6835 - binary_accuracy: 0.5625 - binary_crossentropy: 0.6830\n",
            "Epoch 7/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6825 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6820\n",
            "Epoch 8/250\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6815 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6810\n",
            "Epoch 9/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6806 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6800\n",
            "Epoch 10/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6796 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6791\n",
            "Epoch 11/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6787 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6781\n",
            "Epoch 12/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6777 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6771\n",
            "Epoch 13/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6768 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6762\n",
            "Epoch 14/250\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6758 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6752\n",
            "Epoch 15/250\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6749 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6742\n",
            "Epoch 16/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6739 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6732\n",
            "Epoch 17/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6729 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6723\n",
            "Epoch 18/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6720 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6713\n",
            "Epoch 19/250\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6710 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6703\n",
            "Epoch 20/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6700 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6693\n",
            "Epoch 21/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6690 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6683\n",
            "Epoch 22/250\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6680 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6673\n",
            "Epoch 23/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6670 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6663\n",
            "Epoch 24/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6659 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6651\n",
            "Epoch 25/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6635 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6627\n",
            "Epoch 26/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6612 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6603\n",
            "Epoch 27/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6590 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6582\n",
            "Epoch 28/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6569 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6560\n",
            "Epoch 29/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6546 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6536\n",
            "Epoch 30/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6521 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6512\n",
            "Epoch 31/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6495 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6486\n",
            "Epoch 32/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6469 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6459\n",
            "Epoch 33/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6443 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6433\n",
            "Epoch 34/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6418 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6408\n",
            "Epoch 35/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6394 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6383\n",
            "Epoch 36/250\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6369 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6358\n",
            "Epoch 37/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6344 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6333\n",
            "Epoch 38/250\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6318 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6306\n",
            "Epoch 39/250\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6292 - binary_accuracy: 0.5417 - binary_crossentropy: 0.6280\n",
            "Epoch 40/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6265 - binary_accuracy: 0.6250 - binary_crossentropy: 0.6253\n",
            "Epoch 41/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6238 - binary_accuracy: 0.9375 - binary_crossentropy: 0.6226\n",
            "Epoch 42/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6211 - binary_accuracy: 1.0000 - binary_crossentropy: 0.6199\n",
            "Epoch 43/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6184 - binary_accuracy: 1.0000 - binary_crossentropy: 0.6171\n",
            "Epoch 44/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6158 - binary_accuracy: 1.0000 - binary_crossentropy: 0.6144\n",
            "Epoch 45/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6131 - binary_accuracy: 1.0000 - binary_crossentropy: 0.6117\n",
            "Epoch 46/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6104 - binary_accuracy: 1.0000 - binary_crossentropy: 0.6090\n",
            "Epoch 47/250\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6077 - binary_accuracy: 1.0000 - binary_crossentropy: 0.6063\n",
            "Epoch 48/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6050 - binary_accuracy: 1.0000 - binary_crossentropy: 0.6035\n",
            "Epoch 49/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6022 - binary_accuracy: 1.0000 - binary_crossentropy: 0.6007\n",
            "Epoch 50/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5994 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5979\n",
            "Epoch 51/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5966 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5950\n",
            "Epoch 52/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5938 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5922\n",
            "Epoch 53/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5909 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5893\n",
            "Epoch 54/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5880 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5864\n",
            "Epoch 55/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5852 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5834\n",
            "Epoch 56/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5822 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5805\n",
            "Epoch 57/250\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.5793 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5775\n",
            "Epoch 58/250\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5763 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5745\n",
            "Epoch 59/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.5734 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5715\n",
            "Epoch 60/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5704 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5685\n",
            "Epoch 61/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5674 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5655\n",
            "Epoch 62/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5644 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5624\n",
            "Epoch 63/250\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.5614 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5594\n",
            "Epoch 64/250\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5584 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5563\n",
            "Epoch 65/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.5553 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5532\n",
            "Epoch 66/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5523 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5502\n",
            "Epoch 67/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5493 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5471\n",
            "Epoch 68/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5462 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5440\n",
            "Epoch 69/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5432 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5409\n",
            "Epoch 70/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5402 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5379\n",
            "Epoch 71/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5372 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5348\n",
            "Epoch 72/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5342 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5317\n",
            "Epoch 73/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5312 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5287\n",
            "Epoch 74/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5282 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5257\n",
            "Epoch 75/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5252 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5227\n",
            "Epoch 76/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5222 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5197\n",
            "Epoch 77/250\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.5193 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5167\n",
            "Epoch 78/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5164 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5137\n",
            "Epoch 79/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5135 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5108\n",
            "Epoch 80/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5107 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5079\n",
            "Epoch 81/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5078 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5050\n",
            "Epoch 82/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.5050 - binary_accuracy: 1.0000 - binary_crossentropy: 0.5022\n",
            "Epoch 83/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5023 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4993\n",
            "Epoch 84/250\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4995 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4966\n",
            "Epoch 85/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4968 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4938\n",
            "Epoch 86/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4941 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4911\n",
            "Epoch 87/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4915 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4884\n",
            "Epoch 88/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4888 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4857\n",
            "Epoch 89/250\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.4862 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4831\n",
            "Epoch 90/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4837 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4805\n",
            "Epoch 91/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4812 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4779\n",
            "Epoch 92/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4787 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4753\n",
            "Epoch 93/250\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4762 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4728\n",
            "Epoch 94/250\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4738 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4703\n",
            "Epoch 95/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4714 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4679\n",
            "Epoch 96/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4690 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4655\n",
            "Epoch 97/250\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4666 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4631\n",
            "Epoch 98/250\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4643 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4607\n",
            "Epoch 99/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4620 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4583\n",
            "Epoch 100/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4597 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4560\n",
            "Epoch 101/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4575 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4537\n",
            "Epoch 102/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4553 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4515\n",
            "Epoch 103/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4531 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4492\n",
            "Epoch 104/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4509 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4470\n",
            "Epoch 105/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4487 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4448\n",
            "Epoch 106/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4466 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4426\n",
            "Epoch 107/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4445 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4405\n",
            "Epoch 108/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4424 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4384\n",
            "Epoch 109/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4404 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4363\n",
            "Epoch 110/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4384 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4342\n",
            "Epoch 111/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4364 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4322\n",
            "Epoch 112/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4344 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4301\n",
            "Epoch 113/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4324 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4281\n",
            "Epoch 114/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4305 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4262\n",
            "Epoch 115/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4286 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4242\n",
            "Epoch 116/250\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4267 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4223\n",
            "Epoch 117/250\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4248 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4203\n",
            "Epoch 118/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4230 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4184\n",
            "Epoch 119/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4211 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4166\n",
            "Epoch 120/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4193 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4147\n",
            "Epoch 121/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4175 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4129\n",
            "Epoch 122/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4158 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4111\n",
            "Epoch 123/250\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.4140 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4093\n",
            "Epoch 124/250\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4123 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4075\n",
            "Epoch 125/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4106 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4057\n",
            "Epoch 126/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.4089 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4040\n",
            "Epoch 127/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4072 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4022\n",
            "Epoch 128/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4055 - binary_accuracy: 1.0000 - binary_crossentropy: 0.4005\n",
            "Epoch 129/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4039 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3988\n",
            "Epoch 130/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4022 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3971\n",
            "Epoch 131/250\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.4006 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3955\n",
            "Epoch 132/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3990 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3938\n",
            "Epoch 133/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3974 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3922\n",
            "Epoch 134/250\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3958 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3906\n",
            "Epoch 135/250\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3943 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3890\n",
            "Epoch 136/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3928 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3874\n",
            "Epoch 137/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3912 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3858\n",
            "Epoch 138/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3897 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3842\n",
            "Epoch 139/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3882 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3827\n",
            "Epoch 140/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3867 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3812\n",
            "Epoch 141/250\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3853 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3797\n",
            "Epoch 142/250\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3838 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3782\n",
            "Epoch 143/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3824 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3767\n",
            "Epoch 144/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3809 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3752\n",
            "Epoch 145/250\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3795 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3737\n",
            "Epoch 146/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3781 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3723\n",
            "Epoch 147/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3767 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3708\n",
            "Epoch 148/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3754 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3694\n",
            "Epoch 149/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3740 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3680\n",
            "Epoch 150/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3726 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3666\n",
            "Epoch 151/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3713 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3652\n",
            "Epoch 152/250\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3700 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3638\n",
            "Epoch 153/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3687 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3625\n",
            "Epoch 154/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3674 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3611\n",
            "Epoch 155/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3661 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3598\n",
            "Epoch 156/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3648 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3585\n",
            "Epoch 157/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3635 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3571\n",
            "Epoch 158/250\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3622 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3558\n",
            "Epoch 159/250\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3610 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3545\n",
            "Epoch 160/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3598 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3532\n",
            "Epoch 161/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3585 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3520\n",
            "Epoch 162/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3573 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3507\n",
            "Epoch 163/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3561 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3494\n",
            "Epoch 164/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3549 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3482\n",
            "Epoch 165/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3537 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3469\n",
            "Epoch 166/250\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.3525 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3457\n",
            "Epoch 167/250\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3513 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3445\n",
            "Epoch 168/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3502 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3433\n",
            "Epoch 169/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3490 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3421\n",
            "Epoch 170/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3479 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3409\n",
            "Epoch 171/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3467 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3397\n",
            "Epoch 172/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3456 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3385\n",
            "Epoch 173/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3445 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3373\n",
            "Epoch 174/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3433 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3362\n",
            "Epoch 175/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3422 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3350\n",
            "Epoch 176/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3411 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3338\n",
            "Epoch 177/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3400 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3327\n",
            "Epoch 178/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3390 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3316\n",
            "Epoch 179/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3379 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3304\n",
            "Epoch 180/250\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.3368 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3293\n",
            "Epoch 181/250\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3358 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3282\n",
            "Epoch 182/250\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3347 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3271\n",
            "Epoch 183/250\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3336 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3260\n",
            "Epoch 184/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3326 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3249\n",
            "Epoch 185/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3316 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3238\n",
            "Epoch 186/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3305 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3228\n",
            "Epoch 187/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3295 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3217\n",
            "Epoch 188/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3285 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3206\n",
            "Epoch 189/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3275 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3196\n",
            "Epoch 190/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3265 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3185\n",
            "Epoch 191/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3255 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3175\n",
            "Epoch 192/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3245 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3164\n",
            "Epoch 193/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3235 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3154\n",
            "Epoch 194/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3226 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3144\n",
            "Epoch 195/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3216 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3134\n",
            "Epoch 196/250\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.3206 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3123\n",
            "Epoch 197/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3197 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3113\n",
            "Epoch 198/250\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3187 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3103\n",
            "Epoch 199/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3178 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3093\n",
            "Epoch 200/250\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3168 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3084\n",
            "Epoch 201/250\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.3159 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3074\n",
            "Epoch 202/250\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3150 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3064\n",
            "Epoch 203/250\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3140 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3054\n",
            "Epoch 204/250\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3131 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3045\n",
            "Epoch 205/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3122 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3035\n",
            "Epoch 206/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3113 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3025\n",
            "Epoch 207/250\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3104 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3016\n",
            "Epoch 208/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3095 - binary_accuracy: 1.0000 - binary_crossentropy: 0.3006\n",
            "Epoch 209/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3086 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2997\n",
            "Epoch 210/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3077 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2988\n",
            "Epoch 211/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3068 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2978\n",
            "Epoch 212/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3060 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2969\n",
            "Epoch 213/250\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.3051 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2960\n",
            "Epoch 214/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3042 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2951\n",
            "Epoch 215/250\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3034 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2942\n",
            "Epoch 216/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3025 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2932\n",
            "Epoch 217/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3017 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2923\n",
            "Epoch 218/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3008 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2915\n",
            "Epoch 219/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3000 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2906\n",
            "Epoch 220/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2991 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2897\n",
            "Epoch 221/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2983 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2888\n",
            "Epoch 222/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2975 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2879\n",
            "Epoch 223/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2967 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2870\n",
            "Epoch 224/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2958 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2862\n",
            "Epoch 225/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2950 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2853\n",
            "Epoch 226/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2942 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2845\n",
            "Epoch 227/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2934 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2836\n",
            "Epoch 228/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2926 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2828\n",
            "Epoch 229/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2918 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2819\n",
            "Epoch 230/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2910 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2811\n",
            "Epoch 231/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2902 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2802\n",
            "Epoch 232/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2895 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2794\n",
            "Epoch 233/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2887 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2786\n",
            "Epoch 234/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2879 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2777\n",
            "Epoch 235/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2871 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2769\n",
            "Epoch 236/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2864 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2761\n",
            "Epoch 237/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2856 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2753\n",
            "Epoch 238/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2849 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2745\n",
            "Epoch 239/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2841 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2737\n",
            "Epoch 240/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2834 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2729\n",
            "Epoch 241/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2826 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2721\n",
            "Epoch 242/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2819 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2713\n",
            "Epoch 243/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2811 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2705\n",
            "Epoch 244/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2804 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2697\n",
            "Epoch 245/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2797 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2689\n",
            "Epoch 246/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2789 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2682\n",
            "Epoch 247/250\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2782 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2674\n",
            "Epoch 248/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2775 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2666\n",
            "Epoch 249/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2768 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2658\n",
            "Epoch 250/250\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2761 - binary_accuracy: 1.0000 - binary_crossentropy: 0.2651\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LearningShapelets(batch_size=256, max_iter=250, max_size=None,\n",
              "                  n_shapelets_per_size={20: 1},\n",
              "                  optimizer=<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9d54a0fd90>,\n",
              "                  random_state=42, scale=False, shapelet_length=0.15,\n",
              "                  total_lengths=3, verbose=1, weight_regularizer=0.001)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7pUrqEJztTW",
        "outputId": "6dac553c-bd66-4ded-ddd6-8e99fdf960d5"
      },
      "source": [
        "# Get the number of extracted shapelets, the (minimal) distances from\n",
        "# each of the timeseries to each of the shapelets, and the corresponding\n",
        "# locations (index) where the minimal distance was found\n",
        "n_shapelets = sum(shapelet_sizes.values())\n",
        "distances = shp_clf.transform(X_train)\n",
        "predicted_locations = shp_clf.locate(X_train)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "wNeBqz2iz6Nj",
        "outputId": "9284e053-fb31-4ac3-85fe-0190f7795373"
      },
      "source": [
        "f, ax = plt.subplots(2, 1, sharex=True)\n",
        "\n",
        "# Plot the shapelet and align it on the best matched time series. The optimizer\n",
        "# will often enlarge the shapelet to create a larger gap between the distances\n",
        "# of both classes. We therefore normalize the shapelet again before plotting.\n",
        "test_ts_id = numpy.argmin(numpy.sum(distances, axis=1))\n",
        "shap = shp_clf.shapelets_[0]\n",
        "shap = TimeSeriesScalerMinMax().fit_transform(shap.reshape(1, -1, 1)).flatten()\n",
        "pos = predicted_locations[test_ts_id, 0]\n",
        "ax[0].plot(X_train[test_ts_id].ravel())\n",
        "ax[0].plot(numpy.arange(pos, pos + len(shap)), shap, linewidth=2)\n",
        "ax[0].axvline(pos, color='k', linestyle='--', alpha=0.25)\n",
        "ax[0].set_title(\"The aligned extracted shapelet\")\n",
        "\n",
        "# We calculate the distances from the shapelet to the timeseries ourselves.\n",
        "distances = []\n",
        "time_series = X_train[test_ts_id].ravel()\n",
        "for i in range(len(time_series) - len(shap)):\n",
        "    distances.append(numpy.linalg.norm(time_series[i:i+len(shap)] - shap))\n",
        "ax[1].plot(distances)\n",
        "ax[1].axvline(numpy.argmin(distances), color='k', linestyle='--', alpha=0.25)\n",
        "ax[1].set_title('The distances between the time series and the shapelet')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}